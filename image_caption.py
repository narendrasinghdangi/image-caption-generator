# -*- coding: utf-8 -*-
"""image_caption.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RdPWEbUphukunFE-fd63zM0n43MEW-q-
"""

import numpy as np
import pandas as pd
import cv2
from matplotlib import pyplot as plt
from glob import glob
import tensorflow

"""### **Load the Images Data**"""

images=glob("drive/MyDrive/data/Flickr_Data/Images/*.jpg")

len(images)

images[:5]

"""### **Load the Text Data**"""

captions=open("drive/MyDrive/data/Flickr_Data/Flickr_TextData/Flickr8k.token.txt","rb").read().decode("utf-8").split("\n")

len(captions)

captions[:10]

"""### **Image PreProsessing**"""

img=cv2.imread(images[2])

img

img.shape

img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
img=cv2.resize(img,(224,224))

img

img.shape





"""### **Working on resnet50**"""

from tensorflow.keras.applications.resnet50 import ResNet50
from keras.models import Model

incept_model=ResNet50(include_top=True)
incept_model.summary()

last_layer=incept_model.layers[-2].output

from tf.keras.uti

tensorflow.keras.utils.plot_model(incept_model,show_shapes=True)

my_model=Model(inputs=incept_model.input,outputs=last_layer)

my_model.summary()

img=img.reshape(1,224,224,3)

img

pred=my_model.predict(img)

pred.shape

pred





img_name=images[2].split("/")
print(img_name)

images_features={}

count=0
for i in images:
    img=cv2.imread(i)
    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    img=cv2.resize(img,(244,244))
    img=img.reshape(1,244,244,3)

    pred=my_model.predict(img).reshape(2048,)
    img_name=i.split("/")[-1]
    images_features[img_name]=pred
    count=count+1

    if count>149:
        break
    elif count%50==0:
        print(count)

images_features









images_features={}

images_features=np.load("/content/drive/MyDrive/data/Flickr_Data/iamges_feature1500.npy",allow_pickle=True)

images_features=images_features.item()

len(images_features)







captions_dict={}
for i in captions:
  try:
    img_name=i.split("\t")[0][:-2]
    caption=i.split("\t")[1]
    if img_name in images_features:
      if img_name not in captions_dict:
        captions_dict[img_name]=[caption]
      else:
        captions_dict[img_name].append(caption)
  except:
    pass

len(captions_dict)

captions_dict





"""### **Check Image with Caption**"""

for i in range(4):
  plt.figure(figsize=(10,10))
  img_name=images[i]
  img=cv2.imread(img_name)
  img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
  plt.xlabel(captions_dict[img_name.split('/')[-1]])
  plt.imshow(img)

def preprocessed(txt):
    modified = txt.lower()
    modified = 'startofseq ' + modified + ' endofseq'
    return modified

for k,v in captions_dict.items():
    for vv in v:
        captions_dict[k][v.index(vv)] = preprocessed(vv)

captions_dict



"""### **Create Vocabulary**"""

count_words = {}
for k,v in captions_dict.items():
    for vv in v:
        for word in vv.split():
            if word not in count_words:

                count_words[word] = 0

            else:
                count_words[word] += 1

count_words

len(count_words)

count_words["the"]

count = 1
new_dict = {}
for k,v in count_words.items():
      new_dict[k] = count
      count += 1

new_dict

len(new_dict)

new_dict['<OUT>'] = len(new_dict)

captions_backup = captions_dict.copy()

for k, v in captions_dict.items():
    for vv in v:
        encoded = []
        for word in vv.split():
            if word not in new_dict:
                encoded.append(new_dict['<OUT>'])
            else:
                encoded.append(new_dict[word])
        captions_dict[k][v.index(vv)] = encoded

captions_dict

MAX_LEN=0
for k,v in captions_dict.items():
  for vv in v:
    if len(vv)>MAX_LEN:
      MAX_LEN=len(vv)

MAX_LEN

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

VOCAB_SIZE = len(new_dict)

def generator(photo, caption):

    X = []
    y_in = []
    y_out = []

    for k, v in caption.items():
        for vv in v:
            for i in range(1, len(vv)):
                X.append(photo[k])

                in_seq= [vv[:i]]
                out_seq = vv[i]

                in_seq = pad_sequences(in_seq, maxlen=MAX_LEN, padding='post', truncating='post')[0]
                out_seq = to_categorical([out_seq], num_classes=VOCAB_SIZE)[0]

                y_in.append(in_seq)
                y_out.append(out_seq)
    return X, y_in, y_out

X, y_in, y_out = generator(images_features, captions_dict)

for i in range(10):
    print(y_in[i])
    print(y_out[i])

len(X), len(y_in), len(y_out)

X = np.array(X)

X.shape

y_in = np.array(y_in, dtype='float64')

y_in.shape

y_out=np.array(y_out,dtype="float64")

y_out.shape

X[20]

y_in[20]

y_out[20]

embedding_size=128
vocab_size=len(new_dict)

image_model=tensorflow.keras.models.Sequential([
    tensorflow.keras.layers.Dense(128,input_shape=(2048,),activation="relu"),
    tensorflow.keras.layers.RepeatVector(MAX_LEN)
])
image_model.summary()

language_model=tensorflow.keras.models.Sequential([
    tensorflow.keras.layers.Embedding(input_dim=vocab_size,output_dim=128,input_length=MAX_LEN),
    tensorflow.keras.layers.LSTM(256,return_sequences=True),
    tensorflow.keras.layers.TimeDistributed(tensorflow.keras.layers.Dense(128))
])

language_model.summary()

conca=tensorflow.keras.layers.Concatenate()([image_model.output,language_model.output])

x=tensorflow.keras.layers.LSTM(128,return_sequences=True)(conca)
x=tensorflow.keras.layers.LSTM(512,return_sequences=False)(x)
x=tensorflow.keras.layers.Dense(vocab_size)(x)
out=tensorflow.keras.layers.Activation("softmax")(x)
model=tensorflow.keras.models.Model(inputs=[image_model.input,language_model.input], outputs=out)
model.compile(loss="categorical_crossentropy", optimizer="adam",metrics=["accuracy"])
model.summary()



tensorflow.keras.utils.plot_model(model,show_shapes=True)

model.fit([X,y_in],y_out, batch_size=512,epochs=2)



inv_dict={v:k for k,v in new_dict.items()}

def pred(im):
  test_img=cv2.imread(im[0])

  test_img=cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)

  lol=cv2.resize(test_img,(224,224))
  lol=np.reshape(lol,(1,224,224,3))

  test_feature=my_model.predict(lol).reshape(1,2048)

  text_input=["startofseq"]
  count=0
  caption=""
  while count<25:
    count+=1
    encoded=[]
    for i in text_input:
      encoded.append(new_dict[i])
    encoded=[encoded]
    encoded=pad_sequences(encoded, padding="post", truncating="post", maxlen=MAX_LEN)
    prediction=np.argmax(model.predict([test_feature, encoded]))
    sampled_word=inv_dict[prediction]
    caption=caption+" "+sampled_word
    if sampled_word=="endofseq":
      break
    text_input.append(sampled_word)

  return caption[:-9]

img_name=glob("/content/drive/MyDrive/Photo /IMG_20220505_213833.jpg")

pred(img_name)

